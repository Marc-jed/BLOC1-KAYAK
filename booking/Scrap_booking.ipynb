{"cells":[{"cell_type":"markdown","metadata":{"id":"hRObT5dxtgu0"},"source":["# Become a movie director\n","\n","Let's use Scrapy to get some information about the 250 top rated movies on <a href=\"http://www.imdb.com/\" target=\"_blank\">IMDB</a>.\n","\n","1. Install `Scrapy`:"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"psZM2Or5Nhd7","executionInfo":{"status":"ok","timestamp":1742834386687,"user_tz":-60,"elapsed":24291,"user":{"displayName":"Gdleds Led","userId":"04477971106892264481"}},"outputId":"0b06874a-c9d7-4b3d-e5d7-473042008cf5"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# import os\n","# # Définir le bon dossier\n","# WORK_DIR = \"/chemin/vers/le/repo/du/notebook\"  # Mets le bon chemin ici\n","# os.chdir(WORK_DIR)\n","\n","# print(\"Répertoire courant :\", os.getcwd())"],"metadata":{"id":"vs03l2NMOlIC","executionInfo":{"status":"ok","timestamp":1742837597822,"user_tz":-60,"elapsed":26,"user":{"displayName":"Gdleds Led","userId":"04477971106892264481"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"23Mz67dMtgu4","executionInfo":{"status":"ok","timestamp":1742837693959,"user_tz":-60,"elapsed":11563,"user":{"displayName":"Gdleds Led","userId":"04477971106892264481"}},"outputId":"9cef486c-0db7-473e-fea7-bdc372b2a656"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scrapy in /usr/local/lib/python3.11/dist-packages (2.12.0)\n","Requirement already satisfied: Twisted>=21.7.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.11.0)\n","Requirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (43.0.3)\n","Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.3.0)\n","Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.3.2)\n","Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.10.0)\n","Requirement already satisfied: pyOpenSSL>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.1)\n","Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.7.0)\n","Requirement already satisfied: service-identity>=18.1.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.0)\n","Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (2.3.1)\n","Requirement already satisfied: zope.interface>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (7.2)\n","Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.4.0)\n","Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.11.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2)\n","Requirement already satisfied: tldextract in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.1.3)\n","Requirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.3.1)\n","Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.7.1)\n","Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.11/dist-packages (from scrapy) (2.0.7)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n","Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.11/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (25.3.0)\n","Requirement already satisfied: pyasn1 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.1)\n","Requirement already satisfied: automat>=24.8.0 in /usr/local/lib/python3.11/dist-packages (from Twisted>=21.7.0->scrapy) (24.8.1)\n","Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.11/dist-packages (from Twisted>=21.7.0->scrapy) (23.10.4)\n","Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.11/dist-packages (from Twisted>=21.7.0->scrapy) (21.0.0)\n","Requirement already satisfied: incremental>=24.7.0 in /usr/local/lib/python3.11/dist-packages (from Twisted>=21.7.0->scrapy) (24.7.2)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from Twisted>=21.7.0->scrapy) (4.12.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface>=5.1.0->scrapy) (75.1.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.10)\n","Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (2.32.3)\n","Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (2.1.0)\n","Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.18.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2025.1.31)\n","Requirement already satisfied: scrapy in /usr/local/lib/python3.11/dist-packages (2.12.0)\n","Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.30.0)\n","Requirement already satisfied: Twisted>=21.7.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.11.0)\n","Requirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (43.0.3)\n","Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.3.0)\n","Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.3.2)\n","Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.10.0)\n","Requirement already satisfied: pyOpenSSL>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.1)\n","Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.7.0)\n","Requirement already satisfied: service-identity>=18.1.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.0)\n","Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (2.3.1)\n","Requirement already satisfied: zope.interface>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (7.2)\n","Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.4.0)\n","Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.11.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2)\n","Requirement already satisfied: tldextract in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.1.3)\n","Requirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.3.1)\n","Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.7.1)\n","Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.11/dist-packages (from scrapy) (2.0.7)\n","Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n","Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.29.0)\n","Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n","Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.1.31)\n","Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.12.2)\n","Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n","Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.11/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (25.3.0)\n","Requirement already satisfied: pyasn1 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.1)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n","Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n","Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n","Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n","Requirement already satisfied: automat>=24.8.0 in /usr/local/lib/python3.11/dist-packages (from Twisted>=21.7.0->scrapy) (24.8.1)\n","Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.11/dist-packages (from Twisted>=21.7.0->scrapy) (23.10.4)\n","Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.11/dist-packages (from Twisted>=21.7.0->scrapy) (21.0.0)\n","Requirement already satisfied: incremental>=24.7.0 in /usr/local/lib/python3.11/dist-packages (from Twisted>=21.7.0->scrapy) (24.7.2)\n","Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface>=5.1.0->scrapy) (75.1.0)\n","Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (2.32.3)\n","Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (2.1.0)\n","Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.18.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.1)\n","Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"]}],"source":["!pip install scrapy\n","!pip install scrapy selenium\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rpJRH13xNfCP"},"outputs":[],"source":["import os\n","import logging\n"]},{"cell_type":"markdown","metadata":{"id":"gSW5zr3KNfCQ"},"source":["2. Create a folder where you will store the scripts and results of this exercise. Create a file named `imdb1.py` in that folder."]},{"cell_type":"markdown","metadata":{"id":"uRvjrYjDNfCQ"},"source":["3. Start by writing code in the script to:\n","* import os, logging, scrapy, and CrawlerProcess from scrapy.crawler"]},{"cell_type":"markdown","metadata":{"id":"M_OyH7T1NfCS"},"source":["4. Create a first spider called `imdb_spider` with:\n","* name `imdb`\n","* start_urls `https://www.imdb.com/chart/boxoffice`\n","\n","This spider should scrape the ranking, the title, the url, the total earnings, the rating and the number of voters for the first movie of the charts.\n","\n","Set up the `CrawlerProcess`.\n","Save the results to a `imdb1.json` file."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gK4d7KYNNfCS","executionInfo":{"status":"ok","timestamp":1742837803057,"user_tz":-60,"elapsed":3348,"user":{"displayName":"Gdleds Led","userId":"04477971106892264481"}},"outputId":"4ff2411e-a4a5-4a53-de0d-2ddc259aed80"},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-24 17:36:43 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: scrapybot)\n","2025-03-24 17:36:43 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.3.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.3, Platform Linux-6.1.85+-x86_64-with-glibc2.35\n","2025-03-24 17:36:43 [scrapy.addons] INFO: Enabled addons:\n","[]\n","2025-03-24 17:36:43 [scrapy.extensions.telnet] INFO: Telnet Password: f4383a4c48e5f7a3\n","2025-03-24 17:36:43 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.feedexport.FeedExporter',\n"," 'scrapy.extensions.logstats.LogStats']\n","2025-03-24 17:36:43 [scrapy.crawler] INFO: Overridden settings:\n","{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n","2025-03-24 17:36:43 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2025-03-24 17:36:43 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2025-03-24 17:36:43 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2025-03-24 17:36:43 [scrapy.core.engine] INFO: Spider opened\n","2025-03-24 17:36:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2025-03-24 17:36:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2025-03-24 17:36:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.booking.com/city/fr/toulouse.fr.html?label=yho748jc-1DCAEoggI46AdIM1gDaE2IAQGYAQ24ARjIAQzYAQPoAQGIAgGoAgS4AsGXhr8GwAIB0gIkMTAyMTFjNTQtNTY5Yy00MDc4LWJiZDAtYWRhOTE1NTE1NjJl2AIE4AIB&sid=db07ae730160671a2cb436e0551dcf0e&aid=397646> (referer: None)\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/parsel/selector.py\", line 619, in xpath\n","    result = xpathev(\n","             ^^^^^^^^\n","  File \"src/lxml/etree.pyx\", line 1624, in lxml.etree._Element.xpath\n","  File \"src/lxml/xpath.pxi\", line 290, in lxml.etree.XPathElementEvaluator.__call__\n","  File \"src/lxml/xpath.pxi\", line 210, in lxml.etree._XPathEvaluatorBase._handle_result\n","lxml.etree.XPathEvalError: Invalid expression\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/scrapy/utils/defer.py\", line 327, in iter_errback\n","    yield next(it)\n","          ^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/scrapy/utils/python.py\", line 368, in __next__\n","    return next(self.data)\n","           ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/scrapy/utils/python.py\", line 368, in __next__\n","    return next(self.data)\n","           ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n","    yield from iterable\n","  File \"/usr/local/lib/python3.11/dist-packages/scrapy/spidermiddlewares/referer.py\", line 379, in <genexpr>\n","    return (self._set_referer(r, response) for r in result)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n","    yield from iterable\n","  File \"/usr/local/lib/python3.11/dist-packages/scrapy/spidermiddlewares/urllength.py\", line 57, in <genexpr>\n","    return (r for r in result if self._filter(r, spider))\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n","    yield from iterable\n","  File \"/usr/local/lib/python3.11/dist-packages/scrapy/spidermiddlewares/depth.py\", line 54, in <genexpr>\n","    return (r for r in result if self._filter(r, response, spider))\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n","    yield from iterable\n","  File \"/content/drive/MyDrive/Colab Notebooks/Data Full Stack/Visual Code/Python/KAYAK/booking/src/booking2.py\", line 24, in parse\n","    name = list_hotel.xpath('.//div[@data-testid=\"title\"]h3/div/text()').get().strip()\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/parsel/selector.py\", line 626, in xpath\n","    raise ValueError(f\"XPath error: {exc} in {query}\")\n","ValueError: XPath error: Invalid expression in .//div[@data-testid=\"title\"]h3/div/text()\n","2025-03-24 17:36:44 [scrapy.core.engine] INFO: Closing spider (finished)\n","2025-03-24 17:36:44 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: /content/drive/MyDrive/Colab Notebooks/Data Full Stack/Visual Code/Python/KAYAK/booking/src/booking_test.json\n","2025-03-24 17:36:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 412,\n"," 'downloader/request_count': 1,\n"," 'downloader/request_method_count/GET': 1,\n"," 'downloader/response_bytes': 252306,\n"," 'downloader/response_count': 1,\n"," 'downloader/response_status_count/200': 1,\n"," 'elapsed_time_seconds': 1.643029,\n"," 'feedexport/success_count/FileFeedStorage': 1,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2025, 3, 24, 17, 36, 44, 805203, tzinfo=datetime.timezone.utc),\n"," 'httpcompression/response_bytes': 1198446,\n"," 'httpcompression/response_count': 1,\n"," 'items_per_minute': None,\n"," 'log_count/ERROR': 1,\n"," 'log_count/INFO': 11,\n"," 'memusage/max': 104857600,\n"," 'memusage/startup': 104857600,\n"," 'response_received_count': 1,\n"," 'responses_per_minute': None,\n"," 'scheduler/dequeued': 1,\n"," 'scheduler/dequeued/memory': 1,\n"," 'scheduler/enqueued': 1,\n"," 'scheduler/enqueued/memory': 1,\n"," 'spider_exceptions/ValueError': 1,\n"," 'start_time': datetime.datetime(2025, 3, 24, 17, 36, 43, 162174, tzinfo=datetime.timezone.utc)}\n","2025-03-24 17:36:44 [scrapy.core.engine] INFO: Spider closed (finished)\n"]}],"source":["!python /content/drive/MyDrive/\"Colab Notebooks/Data Full Stack\"/\"Visual Code\"/Python/KAYAK/booking/src/booking2.py"]},{"cell_type":"markdown","metadata":{"id":"OH4m24UCNfCT"},"source":["5. Create a new script called `imdb2.py` where you'll scrape the same information for all the movies in the chart!"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"icjCZz5xNfCT","executionInfo":{"status":"ok","timestamp":1742835273739,"user_tz":-60,"elapsed":22,"user":{"displayName":"Gdleds Led","userId":"04477971106892264481"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ZjYfMiJENfCU"},"source":["## Part 2 - Optional 💪💪\n","6. Based on the results obtained, create a list called `url_list` containing all the urls for the movies in the charts."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qJrk5Fh0NfCU","outputId":"b3405746-c4bf-4733-f59e-929bbca31a59"},"outputs":[{"data":{"text/plain":["[{'ranking': '1',\n","  'title': 'The Nun II',\n","  'url': '/title/tt10160976/?ref_=chtbo_t_1',\n","  'total_earnings': '$70M',\n","  'rating': '5.9',\n","  'nb_voters': '19K'},\n"," {'ranking': '2',\n","  'title': 'Expend4bles',\n","  'url': '/title/tt3291150/?ref_=chtbo_t_2',\n","  'total_earnings': '$8.7M',\n","  'rating': '5.2',\n","  'nb_voters': '5K'},\n"," {'ranking': '3',\n","  'title': 'A Haunting in Venice',\n","  'url': '/title/tt22687790/?ref_=chtbo_t_3',\n","  'total_earnings': '$26M',\n","  'rating': '6.8',\n","  'nb_voters': '20K'}]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XV1sjQa-NfCU","outputId":"0ac5ba8b-92ec-4ba1-81c9-9971580e229b"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-09-27 17:04:16 [numexpr.utils] INFO: NumExpr defaulting to 8 threads.\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ranking</th>\n","      <th>title</th>\n","      <th>url</th>\n","      <th>total_earnings</th>\n","      <th>rating</th>\n","      <th>nb_voters</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>The Nun II</td>\n","      <td>/title/tt10160976/?ref_=chtbo_t_1</td>\n","      <td>$70M</td>\n","      <td>5.9</td>\n","      <td>19K</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Expend4bles</td>\n","      <td>/title/tt3291150/?ref_=chtbo_t_2</td>\n","      <td>$8.7M</td>\n","      <td>5.2</td>\n","      <td>5K</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>A Haunting in Venice</td>\n","      <td>/title/tt22687790/?ref_=chtbo_t_3</td>\n","      <td>$26M</td>\n","      <td>6.8</td>\n","      <td>20K</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>The Equalizer 3</td>\n","      <td>/title/tt17024450/?ref_=chtbo_t_4</td>\n","      <td>$82M</td>\n","      <td>7.1</td>\n","      <td>21K</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Barbie</td>\n","      <td>/title/tt1517268/?ref_=chtbo_t_5</td>\n","      <td>$631M</td>\n","      <td>7.1</td>\n","      <td>334K</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  ranking                 title                                url  \\\n","0       1            The Nun II  /title/tt10160976/?ref_=chtbo_t_1   \n","1       2           Expend4bles   /title/tt3291150/?ref_=chtbo_t_2   \n","2       3  A Haunting in Venice  /title/tt22687790/?ref_=chtbo_t_3   \n","3       4       The Equalizer 3  /title/tt17024450/?ref_=chtbo_t_4   \n","4       5                Barbie   /title/tt1517268/?ref_=chtbo_t_5   \n","\n","  total_earnings rating nb_voters  \n","0           $70M    5.9       19K  \n","1          $8.7M    5.2        5K  \n","2           $26M    6.8       20K  \n","3           $82M    7.1       21K  \n","4          $631M    7.1      334K  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nl3QCP8kNfCV","outputId":"53438db6-f150-4086-e0e4-7a6f78935021"},"outputs":[{"name":"stdout","output_type":"stream","text":["url list: ['https://www.imdb.com/title/tt10160976/?ref_=chtbo_t_1', 'https://www.imdb.com/title/tt3291150/?ref_=chtbo_t_2', 'https://www.imdb.com/title/tt22687790/?ref_=chtbo_t_3']\n"]}],"source":[]},{"cell_type":"markdown","metadata":{"id":"Y4zjETw8NfCV"},"source":["7. Store the list of urls to a file called `url_list.txt`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Co9-nefGNfCV"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ZQFXhjOTNfCV"},"source":["8. Scrape all the pages from the movies in the charts and extract the complete cast, the Storyline, and the genres, also save the title and url for future joins purposes. *Pay attention to inconsistencies between pages*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b8nvWFcKNfCV","outputId":"f14cd45c-9139-4988-da22-dd9f27ecc756"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-09-27 17:04:47 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: scrapybot)\n","2023-09-27 17:04:47 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.9.16 (main, Jan 11 2023, 16:05:54) - [GCC 11.2.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1t  7 Feb 2023), cryptography 38.0.4, Platform Linux-5.10.102.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\n","2023-09-27 17:04:47 [scrapy.addons] INFO: Enabled addons:\n","[]\n","2023-09-27 17:04:47 [py.warnings] WARNING: /home/antoineco/miniconda3/lib/python3.9/site-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n","\n","It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n","\n","See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n","  return cls(crawler)\n","\n","2023-09-27 17:04:47 [scrapy.extensions.telnet] INFO: Telnet Password: d1ce3247c5f87d50\n","2023-09-27 17:04:47 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.feedexport.FeedExporter',\n"," 'scrapy.extensions.logstats.LogStats']\n","2023-09-27 17:04:47 [scrapy.crawler] INFO: Overridden settings:\n","{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n","2023-09-27 17:04:47 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2023-09-27 17:04:47 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2023-09-27 17:04:47 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2023-09-27 17:04:47 [scrapy.core.engine] INFO: Spider opened\n","2023-09-27 17:04:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2023-09-27 17:04:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2023-09-27 17:04:51 [scrapy.core.engine] INFO: Closing spider (finished)\n","2023-09-27 17:04:51 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: 01-Become_a_movie_director/imdb3.json\n","2023-09-27 17:04:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 2308,\n"," 'downloader/request_count': 10,\n"," 'downloader/request_method_count/GET': 10,\n"," 'downloader/response_bytes': 1816339,\n"," 'downloader/response_count': 10,\n"," 'downloader/response_status_count/200': 10,\n"," 'elapsed_time_seconds': 3.721095,\n"," 'feedexport/success_count/FileFeedStorage': 1,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2023, 9, 27, 15, 4, 51, 506967, tzinfo=datetime.timezone.utc),\n"," 'httpcompression/response_bytes': 10733090,\n"," 'httpcompression/response_count': 10,\n"," 'item_scraped_count': 10,\n"," 'log_count/INFO': 11,\n"," 'log_count/WARNING': 1,\n"," 'memusage/max': 118042624,\n"," 'memusage/startup': 118042624,\n"," 'response_received_count': 10,\n"," 'scheduler/dequeued': 10,\n"," 'scheduler/dequeued/memory': 10,\n"," 'scheduler/enqueued': 10,\n"," 'scheduler/enqueued/memory': 10,\n"," 'start_time': datetime.datetime(2023, 9, 27, 15, 4, 47, 785872, tzinfo=datetime.timezone.utc)}\n","2023-09-27 17:04:51 [scrapy.core.engine] INFO: Spider closed (finished)\n"]}],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"}},"nbformat":4,"nbformat_minor":0}